{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Encoder - RNN Decoder - Word to Word - With Attention\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import io\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "from rouge import Rouge\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a file and return a string\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Transformation\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'..\\\\data_collection\\\\webpages\\\\PIX2CODE_COMPILED\\\\00150311-A7AE-4804-8B7D-9273687B4FC0.png'\n",
      "b'..\\\\data_collection\\\\webpages\\\\PIX2CODE_COMPILED\\\\00190F39-0DE9-47EB-B0C2-856FDD3ACE62.png'\n",
      "b'..\\\\data_collection\\\\webpages\\\\PIX2CODE_COMPILED\\\\00779BBE-DD64-4909-9909-24F5C8044A7B.png'\n",
      "b'..\\\\data_collection\\\\webpages\\\\PIX2CODE_COMPILED\\\\00CDC9A8-3D73-4291-90EF-49178E408797.png'\n",
      "b'..\\\\data_collection\\\\webpages\\\\PIX2CODE_COMPILED\\\\00E15BB2-5568-4466-BA18-A8A8D34FC61C.png'\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'PIX2CODE_COMPILED'\n",
    "data_dir = '../data_collection/webpages/' + dir_name\n",
    "\n",
    "IMG_WIDTH = 299\n",
    "IMG_HEIGHT = 299\n",
    "\n",
    "# get all image filepaths\n",
    "list_img = tf.data.Dataset.list_files(str(data_dir + \"/*.png\"), shuffle=False)\n",
    "\n",
    "DATASET_SIZE = len(glob.glob(data_dir + \"/*.png\"))\n",
    "\n",
    "for f in list_img.take(5):\n",
    "    print(f.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decodes an image string\n",
    "@tf.function\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequence Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', split=\" \", lower=False)\n",
    "# Create the vocabulary \n",
    "tokenizer.fit_on_texts([load_doc('./DSL.vocab')])\n",
    "\n",
    "# Add padding\n",
    "tokenizer.word_index['<PAD>'] = 0\n",
    "tokenizer.index_word[0] = '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_lookup(word):\n",
    "    return tokenizer.word_index[word.numpy().decode()]\n",
    "\n",
    "# tensorflow way to convert text to sequence with tokenizer\n",
    "@tf.function\n",
    "def text_to_seq(text, tokenizer):\n",
    "    spl = tf.map_fn(lambda t: tf.py_function(func=tokenizer_lookup, inp=[t], Tout=tf.int32), text, dtype=\"int32\")\n",
    "    return spl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts image filepath to its label\n",
    "@tf.function\n",
    "def get_seq(file_path):\n",
    "    # convert from png to gui file\n",
    "    path = tf.strings.regex_replace(file_path, \".png\", \".gui\")\n",
    "    # read contents and return\n",
    "    label = tf.constant(\"<START> \") + tf.io.read_file(path) + tf.constant(\" <END>\")\n",
    "    return tf.strings.split(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '{', 2: '}', 3: '<START>', 4: '<END>', 5: 'Body', 6: 'Block', 7: 'Container', 8: 'Row', 9: 'Column', 10: 'Header', 11: 'Footer', 12: 'Paragraph', 13: 'Image', 14: 'Button', 15: 'Subtitle', 16: 'Title', 17: 'Link', 18: 'TextBox', 19: 'CheckBox', 20: 'RadioBox', 21: 'Range', 0: '<PAD>'}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine and Split Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does all processing per dataset image path\n",
    "@tf.function\n",
    "def process_path(file_path):\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    \n",
    "    seq = get_seq(file_path)\n",
    "    seq = text_to_seq(seq, tokenizer)\n",
    "    return img, seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = list_img.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = dataset.map(lambda _, y: tf.size(y), num_parallel_calls=AUTOTUNE)\n",
    "max_length = max_length.reduce(tf.constant(0), lambda x, y: tf.math.maximum(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  (299, 299, 3)\n",
      "Sequence:  ['<START> Body { Header { Button Link Link Link } Row { Column { Subtitle Paragraph Button } } Row { Column { Subtitle Paragraph Button } Column { Subtitle Paragraph Button } } Row { Column { Subtitle Paragraph Button } Column { Subtitle Paragraph Button } } } <END> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x, y: (x, tf.concat([y, tf.repeat(0, max_length - tf.size(y))], -1)))\n",
    "\n",
    "for image, seq in dataset.take(1):\n",
    "    print(\"Image shape: \", image.numpy().shape)\n",
    "    print(\"Sequence: \", tokenizer.sequences_to_texts([seq.numpy()]))\n",
    "    plt.imshow(image.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(DATASET_SIZE * 0.7)\n",
    "val_size = int(DATASET_SIZE * 0.1)\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size).take(val_size)\n",
    "test_dataset = dataset.skip(train_size + val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "BUFFER_SIZE = 50\n",
    "embedding_dim = 16\n",
    "encoder_units = 128\n",
    "decoder_units = 256\n",
    "num_steps = train_size // BATCH_SIZE\n",
    "num_val_steps = val_size // BATCH_SIZE\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Definition\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.model = tf.keras.models.Sequential()\n",
    "        self.model.add(tf.keras.layers.Conv2D(16, (3, 3), padding='valid', activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3,)))\n",
    "        self.model.add(tf.keras.layers.Conv2D(16, (3,3), activation='relu', padding='same', strides=2))\n",
    "        self.model.add(tf.keras.layers.Dropout(0.2))\n",
    "        self.model.add(tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "        self.model.add(tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding='same', strides=2))\n",
    "        self.model.add(tf.keras.layers.Dropout(0.2))\n",
    "        self.model.add(tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "        self.model.add(tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same', strides=2))\n",
    "        self.model.add(tf.keras.layers.Dropout(0.2))\n",
    "        self.model.add(tf.keras.layers.Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "\n",
    "        self.model.add(tf.keras.layers.Dropout(0.2))\n",
    "        \n",
    "        self.out = tf.keras.layers.Dense(embedding_dim)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.model(x)\n",
    "        \n",
    "        x = tf.reshape(x, (x.shape[0], -1, x.shape[3]))\n",
    "        \n",
    "        x = self.out(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size, max_length):\n",
    "        super(RNN_Encoder, self).__init__()\n",
    "        self.units = units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length)\n",
    "        self.gru = tf.keras.models.Sequential()\n",
    "#         self.gru.add(tf.keras.layers.GRU(self.units,\n",
    "#                                    return_sequences=True,\n",
    "#                                    dropout=0.2,\n",
    "#                                    recurrent_initializer='glorot_uniform'))\n",
    "        self.gru.add(tf.keras.layers.GRU(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   dropout=0.2,\n",
    "                                   recurrent_initializer='glorot_uniform'))\n",
    "        self.gru.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units, activation='relu')))\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.gru(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 64, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_RNN_Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size, max_length):\n",
    "        super(CNN_RNN_Encoder, self).__init__()\n",
    "        self.rnn = RNN_Encoder(embedding_dim, units, vocab_size, max_length)\n",
    "        self.cnn = CNN_Encoder(embedding_dim)\n",
    "        self.rep = tf.keras.layers.RepeatVector(1)\n",
    "        self.features = None\n",
    "        \n",
    "    def call(self, seq, img=None):\n",
    "        if img is None and self.features is None:\n",
    "            print(\"Error, img and features None\")\n",
    "            return\n",
    "        elif img is not None:\n",
    "            self.features = self.cnn(img)\n",
    "            \n",
    "        feat = self.rep(self.features)\n",
    "            \n",
    "        seq = self.rnn(seq)\n",
    "        \n",
    "        x = tf.keras.layers.concatenate([feat, seq])\n",
    "        return x\n",
    "    \n",
    "    def set_features(self, img):\n",
    "        self.features = self.cnn(img)\n",
    "    \n",
    "    def reset_state(self, batch_size):\n",
    "        self.features = None\n",
    "        return self.rnn.reset_state(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        self.gru = tf.keras.models.Sequential()\n",
    "#         self.gru.add(tf.keras.layers.GRU(self.units,\n",
    "#                                    return_sequences=True,\n",
    "#                                    dropout=0.2,\n",
    "#                                    recurrent_initializer='glorot_uniform'))\n",
    "        self.gru.add(tf.keras.layers.GRU(self.units,\n",
    "                                   return_sequences=False,\n",
    "                                   dropout=0.2,\n",
    "                                   recurrent_initializer='glorot_uniform'))\n",
    "        self.fc = tf.keras.layers.Dense(units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):    \n",
    "        x = self.gru(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Encoder_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Encoder_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "#         self.gru1 = tf.keras.layers.GRU(self.units,\n",
    "#                                    return_sequences=True,\n",
    "#                                    dropout=0.2,\n",
    "#                                    recurrent_initializer='glorot_uniform')\n",
    "        self.gru2 = tf.keras.layers.GRU(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "                     \n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "        \n",
    "    def call(self, x, features, hidden):\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        output, state = self.gru2(x)\n",
    "\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "    \n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Encoder_Decoder(embedding_dim, decoder_units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/cnn_rnn_w2w_att/exp/sing/again\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []\n",
    "val_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target, val=False):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<START>']] * target.shape[0], 1)\n",
    "    \n",
    "    if val:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "        total_loss = (loss / int(target.shape[1]))\n",
    "        return loss, total_loss\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.9561\n",
      "Time taken for Batches 5.145526647567749 sec\n",
      "Epoch 1 Batch 20 Loss 1.3139\n",
      "Time taken for Batches 30.859331607818604 sec\n",
      "Epoch 1 Batch 40 Loss 1.0329\n",
      "Time taken for Batches 60.36020755767822 sec\n",
      "Epoch 1 Batch 60 Loss 0.6441\n",
      "Time taken for Batches 87.41900324821472 sec\n",
      "Epoch 1 Batch 80 Loss 0.4105\n",
      "Time taken for Batches 113.49633145332336 sec\n",
      "Epoch 1 Batch 100 Loss 0.3543\n",
      "Time taken for Batches 138.443030834198 sec\n",
      "Epoch 1 Batch 120 Loss 0.3780\n",
      "Time taken for Batches 164.00829005241394 sec\n",
      "Epoch 1 Batch 140 Loss 0.3266\n",
      "Time taken for Batches 189.2238574028015 sec\n",
      "Epoch 1 Loss 0.698450 Val Loss 0.318124\n",
      "Time taken for 1 epoch 238.91754794120789 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.3279\n",
      "Time taken for Batches 2.482907772064209 sec\n",
      "Epoch 2 Batch 20 Loss 0.2431\n",
      "Time taken for Batches 30.872958660125732 sec\n",
      "Epoch 2 Batch 40 Loss 0.2558\n",
      "Time taken for Batches 56.80982685089111 sec\n",
      "Epoch 2 Batch 60 Loss 0.2193\n",
      "Time taken for Batches 81.91407322883606 sec\n",
      "Epoch 2 Batch 80 Loss 0.1945\n",
      "Time taken for Batches 107.04173636436462 sec\n",
      "Epoch 2 Batch 100 Loss 0.1643\n",
      "Time taken for Batches 132.63061690330505 sec\n",
      "Epoch 2 Batch 120 Loss 0.1239\n",
      "Time taken for Batches 158.53321313858032 sec\n",
      "Epoch 2 Batch 140 Loss 0.1133\n",
      "Time taken for Batches 184.93929147720337 sec\n",
      "Epoch 2 Loss 0.203283 Val Loss 0.108394\n",
      "Time taken for 1 epoch 236.94909238815308 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.1069\n",
      "Time taken for Batches 2.380018711090088 sec\n",
      "Epoch 3 Batch 20 Loss 0.1211\n",
      "Time taken for Batches 29.057284832000732 sec\n",
      "Epoch 3 Batch 40 Loss 0.0977\n",
      "Time taken for Batches 57.616010904312134 sec\n",
      "Epoch 3 Batch 60 Loss 0.1046\n",
      "Time taken for Batches 83.77592444419861 sec\n",
      "Epoch 3 Batch 80 Loss 0.0897\n",
      "Time taken for Batches 110.95715308189392 sec\n",
      "Epoch 3 Batch 100 Loss 0.1106\n",
      "Time taken for Batches 136.18325757980347 sec\n",
      "Epoch 3 Batch 120 Loss 0.1162\n",
      "Time taken for Batches 161.21062684059143 sec\n",
      "Epoch 3 Batch 140 Loss 0.0879\n",
      "Time taken for Batches 189.5353138446808 sec\n",
      "Epoch 3 Loss 0.106384 Val Loss 0.104294\n",
      "Time taken for 1 epoch 241.39105319976807 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.1223\n",
      "Time taken for Batches 2.3670430183410645 sec\n",
      "Epoch 4 Batch 20 Loss 0.0904\n",
      "Time taken for Batches 27.17396306991577 sec\n",
      "Epoch 4 Batch 40 Loss 0.0953\n",
      "Time taken for Batches 52.27245807647705 sec\n",
      "Epoch 4 Batch 60 Loss 0.1089\n",
      "Time taken for Batches 77.7825973033905 sec\n",
      "Epoch 4 Batch 80 Loss 0.1074\n",
      "Time taken for Batches 102.89198446273804 sec\n",
      "Epoch 4 Batch 100 Loss 0.1447\n",
      "Time taken for Batches 129.0647795200348 sec\n",
      "Epoch 4 Batch 120 Loss 0.0931\n",
      "Time taken for Batches 154.53515791893005 sec\n",
      "Epoch 4 Batch 140 Loss 0.0961\n",
      "Time taken for Batches 180.5943319797516 sec\n",
      "Epoch 4 Loss 0.101460 Val Loss 0.104037\n",
      "Time taken for 1 epoch 237.60961508750916 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1135\n",
      "Time taken for Batches 2.450834274291992 sec\n",
      "Epoch 5 Batch 20 Loss 0.0912\n",
      "Time taken for Batches 31.76228952407837 sec\n",
      "Epoch 5 Batch 40 Loss 0.1077\n",
      "Time taken for Batches 56.871129274368286 sec\n",
      "Epoch 5 Batch 60 Loss 0.0933\n",
      "Time taken for Batches 84.98739504814148 sec\n",
      "Epoch 5 Batch 80 Loss 0.0975\n",
      "Time taken for Batches 110.10153841972351 sec\n",
      "Epoch 5 Batch 100 Loss 0.0986\n",
      "Time taken for Batches 135.01211285591125 sec\n",
      "Epoch 5 Batch 120 Loss 0.0927\n",
      "Time taken for Batches 160.6089038848877 sec\n",
      "Epoch 5 Batch 140 Loss 0.0923\n",
      "Time taken for Batches 187.7264928817749 sec\n",
      "Epoch 5 Loss 0.099715 Val Loss 0.098729\n",
      "Time taken for 1 epoch 240.2235357761383 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.1048\n",
      "Time taken for Batches 2.358112096786499 sec\n",
      "Epoch 6 Batch 20 Loss 0.0844\n",
      "Time taken for Batches 27.734578132629395 sec\n",
      "Epoch 6 Batch 40 Loss 0.0994\n",
      "Time taken for Batches 54.15694785118103 sec\n",
      "Epoch 6 Batch 60 Loss 0.0964\n",
      "Time taken for Batches 78.90029716491699 sec\n",
      "Epoch 6 Batch 80 Loss 0.0991\n",
      "Time taken for Batches 105.7445740699768 sec\n",
      "Epoch 6 Batch 100 Loss 0.1011\n",
      "Time taken for Batches 132.38000988960266 sec\n",
      "Epoch 6 Batch 120 Loss 0.0834\n",
      "Time taken for Batches 157.68770909309387 sec\n",
      "Epoch 6 Batch 140 Loss 0.1053\n",
      "Time taken for Batches 184.55953574180603 sec\n",
      "Epoch 6 Loss 0.100193 Val Loss 0.095929\n",
      "Time taken for 1 epoch 242.54837894439697 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.1015\n",
      "Time taken for Batches 2.2618906497955322 sec\n",
      "Epoch 7 Batch 20 Loss 0.1000\n",
      "Time taken for Batches 37.720746755599976 sec\n",
      "Epoch 7 Batch 40 Loss 0.1020\n",
      "Time taken for Batches 63.169721603393555 sec\n",
      "Epoch 7 Batch 60 Loss 0.0908\n",
      "Time taken for Batches 88.32548522949219 sec\n",
      "Epoch 7 Batch 80 Loss 0.0964\n",
      "Time taken for Batches 113.60355758666992 sec\n",
      "Epoch 7 Batch 100 Loss 0.0985\n",
      "Time taken for Batches 142.14926481246948 sec\n",
      "Epoch 7 Batch 120 Loss 0.1041\n",
      "Time taken for Batches 168.5511758327484 sec\n",
      "Epoch 7 Batch 140 Loss 0.0896\n",
      "Time taken for Batches 195.92183351516724 sec\n",
      "Epoch 7 Loss 0.098044 Val Loss 0.096595\n",
      "Time taken for 1 epoch 246.94684267044067 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0986\n",
      "Time taken for Batches 2.3670670986175537 sec\n",
      "Epoch 8 Batch 20 Loss 0.1046\n",
      "Time taken for Batches 29.200413465499878 sec\n",
      "Epoch 8 Batch 40 Loss 0.0967\n",
      "Time taken for Batches 55.533838510513306 sec\n",
      "Epoch 8 Batch 60 Loss 0.1032\n",
      "Time taken for Batches 81.54297637939453 sec\n",
      "Epoch 8 Batch 80 Loss 0.0929\n",
      "Time taken for Batches 107.60743951797485 sec\n",
      "Epoch 8 Batch 100 Loss 0.1009\n",
      "Time taken for Batches 132.78551840782166 sec\n",
      "Epoch 8 Batch 120 Loss 0.1017\n",
      "Time taken for Batches 159.18434619903564 sec\n",
      "Epoch 8 Batch 140 Loss 0.1011\n",
      "Time taken for Batches 184.74269556999207 sec\n",
      "Epoch 8 Loss 0.097700 Val Loss 0.094676\n",
      "Time taken for 1 epoch 234.53308081626892 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0981\n",
      "Time taken for Batches 2.4720358848571777 sec\n",
      "Epoch 9 Batch 20 Loss 0.1028\n",
      "Time taken for Batches 27.700923204421997 sec\n",
      "Epoch 9 Batch 40 Loss 0.1050\n",
      "Time taken for Batches 52.85122203826904 sec\n",
      "Epoch 9 Batch 60 Loss 0.0987\n",
      "Time taken for Batches 77.98859548568726 sec\n",
      "Epoch 9 Batch 80 Loss 0.0956\n",
      "Time taken for Batches 103.01173090934753 sec\n",
      "Epoch 9 Batch 100 Loss 0.0918\n",
      "Time taken for Batches 128.03373503684998 sec\n",
      "Epoch 9 Batch 120 Loss 0.0984\n",
      "Time taken for Batches 153.5245087146759 sec\n",
      "Epoch 9 Batch 140 Loss 0.1064\n",
      "Time taken for Batches 181.10664749145508 sec\n",
      "Epoch 9 Loss 0.097289 Val Loss 0.097778\n",
      "Time taken for 1 epoch 231.82219076156616 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.1040\n",
      "Time taken for Batches 2.4254255294799805 sec\n",
      "Epoch 10 Batch 20 Loss 0.1004\n",
      "Time taken for Batches 27.811191082000732 sec\n",
      "Epoch 10 Batch 40 Loss 0.1004\n",
      "Time taken for Batches 53.24973821640015 sec\n",
      "Epoch 10 Batch 60 Loss 0.0949\n",
      "Time taken for Batches 79.3365535736084 sec\n",
      "Epoch 10 Batch 80 Loss 0.0931\n",
      "Time taken for Batches 106.7387535572052 sec\n",
      "Epoch 10 Batch 100 Loss 0.0848\n",
      "Time taken for Batches 132.269469499588 sec\n",
      "Epoch 10 Batch 120 Loss 0.0854\n",
      "Time taken for Batches 158.67695307731628 sec\n",
      "Epoch 10 Batch 140 Loss 0.1006\n",
      "Time taken for Batches 185.37700843811035 sec\n",
      "Epoch 10 Loss 0.097084 Val Loss 0.093736\n",
      "Time taken for 1 epoch 240.60501265525818 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.0955\n",
      "Time taken for Batches 2.5880045890808105 sec\n",
      "Epoch 11 Batch 20 Loss 0.0947\n",
      "Time taken for Batches 30.992151498794556 sec\n",
      "Epoch 11 Batch 40 Loss 0.0942\n",
      "Time taken for Batches 57.03775405883789 sec\n",
      "Epoch 11 Batch 60 Loss 0.1040\n",
      "Time taken for Batches 84.90896892547607 sec\n",
      "Epoch 11 Batch 80 Loss 0.0893\n",
      "Time taken for Batches 110.75243997573853 sec\n",
      "Epoch 11 Batch 100 Loss 0.0947\n",
      "Time taken for Batches 138.11273288726807 sec\n",
      "Epoch 11 Batch 120 Loss 0.1071\n",
      "Time taken for Batches 163.79558539390564 sec\n",
      "Epoch 11 Batch 140 Loss 0.0989\n",
      "Time taken for Batches 188.61115646362305 sec\n",
      "Epoch 11 Loss 0.097397 Val Loss 0.094016\n",
      "Time taken for 1 epoch 237.48003721237183 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.0964\n",
      "Time taken for Batches 2.36029314994812 sec\n",
      "Epoch 12 Batch 20 Loss 0.0971\n",
      "Time taken for Batches 27.968735694885254 sec\n",
      "Epoch 12 Batch 40 Loss 0.0911\n",
      "Time taken for Batches 52.81293606758118 sec\n",
      "Epoch 12 Batch 60 Loss 0.0947\n",
      "Time taken for Batches 77.7779552936554 sec\n",
      "Epoch 12 Batch 80 Loss 0.0962\n",
      "Time taken for Batches 23060.620156288147 sec\n",
      "Epoch 12 Batch 100 Loss 0.0997\n",
      "Time taken for Batches 23089.555168390274 sec\n",
      "Epoch 12 Batch 120 Loss 0.0875\n",
      "Time taken for Batches 23116.05136013031 sec\n",
      "Epoch 12 Batch 140 Loss 0.0865\n",
      "Time taken for Batches 23142.930748462677 sec\n",
      "Epoch 12 Loss 0.097425 Val Loss 0.099989\n",
      "Time taken for 1 epoch 23192.912561416626 sec\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Batch 0 Loss 0.1005\n",
      "Time taken for Batches 2.428288221359253 sec\n",
      "Epoch 13 Batch 20 Loss 0.1035\n",
      "Time taken for Batches 37.145891189575195 sec\n",
      "Epoch 13 Batch 40 Loss 0.0886\n",
      "Time taken for Batches 64.54795932769775 sec\n",
      "Epoch 13 Batch 60 Loss 0.1023\n",
      "Time taken for Batches 89.45547103881836 sec\n",
      "Epoch 13 Batch 80 Loss 0.0944\n",
      "Time taken for Batches 115.12777924537659 sec\n",
      "Epoch 13 Batch 100 Loss 0.1018\n",
      "Time taken for Batches 140.11887121200562 sec\n",
      "Epoch 13 Batch 120 Loss 0.1132\n",
      "Time taken for Batches 165.05753922462463 sec\n",
      "Epoch 13 Batch 140 Loss 0.1015\n",
      "Time taken for Batches 189.8448691368103 sec\n",
      "Epoch 13 Loss 0.097043 Val Loss 0.096703\n",
      "Time taken for 1 epoch 239.26235914230347 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.1071\n",
      "Time taken for Batches 2.306262493133545 sec\n",
      "Epoch 14 Batch 20 Loss 0.0858\n",
      "Time taken for Batches 27.17848825454712 sec\n",
      "Epoch 14 Batch 40 Loss 0.0905\n",
      "Time taken for Batches 52.40693664550781 sec\n",
      "Epoch 14 Batch 60 Loss 0.0996\n",
      "Time taken for Batches 77.37931799888611 sec\n",
      "Epoch 14 Batch 80 Loss 0.0917\n",
      "Time taken for Batches 102.14949584007263 sec\n",
      "Epoch 14 Batch 100 Loss 0.0885\n",
      "Time taken for Batches 127.49090313911438 sec\n",
      "Epoch 14 Batch 120 Loss 0.1046\n",
      "Time taken for Batches 152.30781054496765 sec\n",
      "Epoch 14 Batch 140 Loss 0.1005\n",
      "Time taken for Batches 177.19217443466187 sec\n",
      "Epoch 14 Loss 0.097827 Val Loss 0.095479\n",
      "Time taken for 1 epoch 226.65524053573608 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.1013\n",
      "Time taken for Batches 2.3398427963256836 sec\n",
      "Epoch 15 Batch 20 Loss 0.0975\n",
      "Time taken for Batches 27.232930660247803 sec\n",
      "Epoch 15 Batch 40 Loss 0.0958\n",
      "Time taken for Batches 52.11523222923279 sec\n",
      "Epoch 15 Batch 60 Loss 0.1025\n",
      "Time taken for Batches 76.93578743934631 sec\n",
      "Epoch 15 Batch 80 Loss 0.0850\n",
      "Time taken for Batches 101.76387214660645 sec\n",
      "Epoch 15 Batch 100 Loss 0.0914\n",
      "Time taken for Batches 126.59697556495667 sec\n",
      "Epoch 15 Batch 120 Loss 0.0947\n",
      "Time taken for Batches 151.4662766456604 sec\n",
      "Epoch 15 Batch 140 Loss 0.0716\n",
      "Time taken for Batches 176.48945546150208 sec\n",
      "Epoch 15 Loss 0.096729 Val Loss 0.094877\n",
      "Time taken for 1 epoch 225.82201147079468 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.1037\n",
      "Time taken for Batches 2.2219245433807373 sec\n",
      "Epoch 16 Batch 20 Loss 0.1026\n",
      "Time taken for Batches 26.368938446044922 sec\n",
      "Epoch 16 Batch 40 Loss 0.0963\n",
      "Time taken for Batches 50.4245982170105 sec\n",
      "Epoch 16 Batch 60 Loss 0.1045\n",
      "Time taken for Batches 74.778076171875 sec\n",
      "Epoch 16 Batch 80 Loss 0.0869\n",
      "Time taken for Batches 98.77043914794922 sec\n",
      "Epoch 16 Batch 100 Loss 0.1008\n",
      "Time taken for Batches 122.6161150932312 sec\n",
      "Epoch 16 Batch 120 Loss 0.0976\n",
      "Time taken for Batches 146.6789584159851 sec\n",
      "Epoch 16 Batch 140 Loss 0.0921\n",
      "Time taken for Batches 170.63482093811035 sec\n",
      "Epoch 16 Loss 0.095087 Val Loss 0.092730\n",
      "Time taken for 1 epoch 218.14307117462158 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.0968\n",
      "Time taken for Batches 2.259810447692871 sec\n",
      "Epoch 17 Batch 20 Loss 0.1015\n",
      "Time taken for Batches 26.496406316757202 sec\n",
      "Epoch 17 Batch 40 Loss 0.0967\n",
      "Time taken for Batches 50.41783404350281 sec\n",
      "Epoch 17 Batch 60 Loss 0.0960\n",
      "Time taken for Batches 75.20986723899841 sec\n",
      "Epoch 17 Batch 80 Loss 0.0931\n",
      "Time taken for Batches 99.17296028137207 sec\n",
      "Epoch 17 Batch 100 Loss 0.0972\n",
      "Time taken for Batches 123.14252257347107 sec\n",
      "Epoch 17 Batch 120 Loss 0.0955\n",
      "Time taken for Batches 147.04317164421082 sec\n",
      "Epoch 17 Batch 140 Loss 0.1006\n",
      "Time taken for Batches 170.96500825881958 sec\n",
      "Epoch 17 Loss 0.095060 Val Loss 0.092452\n",
      "Time taken for 1 epoch 217.77257919311523 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.0986\n",
      "Time taken for Batches 2.3375306129455566 sec\n",
      "Epoch 18 Batch 20 Loss 0.0955\n",
      "Time taken for Batches 26.194390296936035 sec\n",
      "Epoch 18 Batch 40 Loss 0.0882\n",
      "Time taken for Batches 50.02692723274231 sec\n",
      "Epoch 18 Batch 60 Loss 0.0990\n",
      "Time taken for Batches 73.89627981185913 sec\n",
      "Epoch 18 Batch 80 Loss 0.1049\n",
      "Time taken for Batches 97.90198636054993 sec\n",
      "Epoch 18 Batch 100 Loss 0.0942\n",
      "Time taken for Batches 121.940256357193 sec\n",
      "Epoch 18 Batch 120 Loss 0.0951\n",
      "Time taken for Batches 146.43462419509888 sec\n",
      "Epoch 18 Batch 140 Loss 0.0965\n",
      "Time taken for Batches 171.9965159893036 sec\n",
      "Epoch 18 Loss 0.094947 Val Loss 0.092893\n",
      "Time taken for 1 epoch 221.469975233078 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.0849\n",
      "Time taken for Batches 2.3377130031585693 sec\n",
      "Epoch 19 Batch 20 Loss 0.0889\n",
      "Time taken for Batches 27.39814066886902 sec\n",
      "Epoch 19 Batch 40 Loss 0.0932\n",
      "Time taken for Batches 52.630550146102905 sec\n",
      "Epoch 19 Batch 60 Loss 0.1033\n",
      "Time taken for Batches 77.66442656517029 sec\n",
      "Epoch 19 Batch 80 Loss 0.0900\n",
      "Time taken for Batches 102.69118666648865 sec\n",
      "Epoch 19 Batch 100 Loss 0.1026\n",
      "Time taken for Batches 127.67653489112854 sec\n",
      "Epoch 19 Batch 120 Loss 0.0943\n",
      "Time taken for Batches 152.7570993900299 sec\n",
      "Epoch 19 Batch 140 Loss 0.0956\n",
      "Time taken for Batches 177.70356559753418 sec\n",
      "Epoch 19 Loss 0.094954 Val Loss 0.092002\n",
      "Time taken for 1 epoch 228.36361002922058 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0930\n",
      "Time taken for Batches 2.4744210243225098 sec\n",
      "Epoch 20 Batch 20 Loss 0.1024\n",
      "Time taken for Batches 27.4650137424469 sec\n",
      "Epoch 20 Batch 40 Loss 0.0949\n",
      "Time taken for Batches 51.662067890167236 sec\n",
      "Epoch 20 Batch 60 Loss 0.0927\n",
      "Time taken for Batches 75.82473850250244 sec\n",
      "Epoch 20 Batch 80 Loss 0.0958\n",
      "Time taken for Batches 99.96154880523682 sec\n",
      "Epoch 20 Batch 100 Loss 0.0889\n",
      "Time taken for Batches 123.98753356933594 sec\n",
      "Epoch 20 Batch 120 Loss 0.0972\n",
      "Time taken for Batches 149.21818280220032 sec\n",
      "Epoch 20 Batch 140 Loss 0.0847\n",
      "Time taken for Batches 174.19807767868042 sec\n",
      "Epoch 20 Loss 0.095361 Val Loss 0.092929\n",
      "Time taken for 1 epoch 224.89532113075256 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    total_val_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 20 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "            epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "            print ('Time taken for Batches {} sec'.format(time.time() - start))\n",
    "            \n",
    "    for (batch, (img_tensor, target)) in enumerate(val_dataset):\n",
    "        _, t_loss = train_step(img_tensor, target, True)\n",
    "        total_val_loss += t_loss\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "    val_plot.append(total_val_loss / num_val_steps)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f} Val Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps, total_val_loss/num_val_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(plot_path, plot_list):\n",
    "    with open(plot_path, 'wb') as fp:\n",
    "        pickle.dump(plot_list, fp)\n",
    "        \n",
    "def load_plot(plot_path):\n",
    "    with open (plot_path, 'rb') as fp:\n",
    "         return pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.plot(val_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.legend(['Loss', 'Val Loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, 1444))\n",
    "    \n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    features = encoder(tf.expand_dims(image, 0))\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<START>']], 0)\n",
    "    result = ['<START>']\n",
    "\n",
    "    for i in range(1, max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "        \n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<END>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_path(imgpath):\n",
    "    return evaluate(process_img_path(imgpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot, lim=None):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    if lim is None:\n",
    "        lim = len(result)\n",
    "\n",
    "    for l in range(lim):\n",
    "        temp_att = np.resize(attention_plot[l], (38, 38))\n",
    "        ax = fig.add_subplot(lim//2, lim//2, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_entry = test_dataset.take(1)\n",
    "\n",
    "for image, seq in test_entry:\n",
    "    actual = tokenizer.sequences_to_texts([seq.numpy()])[0].replace(\"<PAD>\", \"\")\n",
    "    predicted, attention = evaluate(image)\n",
    "    predicted = ' '.join(predicted)\n",
    "    print(\"Actual DSL: \", actual)\n",
    "    print(\"\\nPredicted DSL: \", predicted)\n",
    "    plot_attention(image.numpy(), predicted.split(), attention, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(dataset, verbose=False):\n",
    "    actual, predicted = list(), list()\n",
    "    i = 0\n",
    "    for image, seq in dataset:\n",
    "        act = tokenizer.sequences_to_texts([seq.numpy()])[0].replace(\"<PAD>\", \"\").split()\n",
    "        pred, _ = evaluate(image)\n",
    "        pred = [\"<START>\"] + pred\n",
    "        if verbose:\n",
    "            print(\"\\n\\nActual DSL---->\\n\", act)\n",
    "            print(\"\\nPredicted DSL---->\\n\", pred)\n",
    "        else:\n",
    "            print(\"Predicted count: \" + str(i + 1), end='\\r', flush=True)\n",
    "        actual.append([act])\n",
    "        predicted.append(pred)\n",
    "        i += 1\n",
    "    bleu = corpus_bleu(actual, predicted)\n",
    "    \n",
    "    _actual = list(map(lambda p: ' '.join(p[0]), actual))\n",
    "    _predicted = list(map(lambda p: ' '.join(p), predicted))\n",
    "    _rouge = Rouge()\n",
    "    rouge = _rouge.get_scores(_predicted, _actual, avg=True)\n",
    "    return bleu, rouge,  actual, predicted\n",
    "        \n",
    "bleu, rouge, actual, predicted = score(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bleu)\n",
    "print(rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
